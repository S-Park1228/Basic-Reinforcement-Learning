{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.part3.MLP import MultiLayerPerceptron as MLP\n",
    "from src.common.linear_data import generate_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 것의 시작 `nn.Module` \n",
    "\n",
    "`nn.Module`은 앞으로 여러분들이 심층신경망을 `pytorch`로 구현할 때, 거의 모든 경우 base class로 사용하게 될 모듈입니다. 예제를 확인하면서 어떻게 `nn.Module` 를 사용하는지 확인해보도록 하죠.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_neurons: list,\n",
    "                 hidden_act: str,\n",
    "                 out_act: str):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        self.hidden_act = getattr(nn, hidden_act)()\n",
    "        self.out_act = getattr(nn, out_act)()\n",
    "\n",
    "        input_dims = [input_dim] + num_neurons\n",
    "        output_dims = num_neurons + [output_dim]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            is_last = True if i == len(input_dims) else False\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if is_last:\n",
    "                self.layers.append(self.out_act)\n",
    "            else:\n",
    "                self.layers.append(self.hidden_act)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer(xs)\n",
    "        return xs\n",
    "```\n",
    "\n",
    "아주 아주 특별한 경우가 아니라면, 여러분들께서는 자신만의 심층신경망을 만들 때 `nn.Module`을 상속받은 후 최소 두가지를 구현하게 되실거에요.\n",
    "\n",
    "### 1. `__init__`\n",
    "\n",
    "__init__ 은 파이썬의 class constructor 이죠? 항상 컨스트럭터의 첫줄에는\n",
    "```python\n",
    "super(MultiLayerPerceptron, self).__init__()\n",
    "```\n",
    "과 같은 형태로 parent class, 즉 `nn.module`을 초기화 해주세요.\n",
    "만약 그렇지 않으면, 뒤에 설명드릴 `nn.moudle`의 강력한 기능을 잃게 됩니다.\n",
    "### 2.`forward()`\n",
    "forward()는 심층신경망의 연산 과정을 정의하는 함수입니다. 위의 사례에서는 각 레이어들 (`linear`, `activation`)을 번갈아가면서 사용해서 MLP의 연산과정을 구현했던 것을 확인해보면 좋겠네요.\n",
    "\n",
    "`nn.module`은 `forward()`로 `__call__()` overriding 합니다. 보기에 좋으라고 그냥 오버라이딩을 하는건 아니고, `nn.module`의 내부에서 `pre_forward_hook`, `post_forward_hook`등을 활용해서 automatic differentiation 에 필요한 기능등을 보조해줍니다.\n",
    "\n",
    "따라서! 아주 특별한 경우가 아니라면 `forward()`를 구현하고 심층신경망 연산을 진행할때도 `forward()`함수를 사용하는 것을 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module` 에 대한 조금은 deep한 이야기\n",
    "\n",
    "`nn.Module`은 생각보다 훨씬 더 강력한 편의기능들을 가지고 있습니다. 몇몇 가지 중요한 특징 및 기능에 대해서 알아보도록 하겠습니다.\n",
    "\n",
    "> `nn.Module`에 __등록되는__ `파라미터` 및 다른 `nn.Module`은 `nn.Module`의 `childrun` 으로 관리된다. <br> ~\"아니 이게 무슨 소리야\"~\n",
    "\n",
    "주) `pytorch` 에서 `파라미터`는 미분을 계산해야하는 텐서들을 지칭합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()        \n",
    "        self.layer1 = nn.Linear(5,2)\n",
    "        self.layer2 = nn.Linear(5,2)\n",
    "        # 학습중 업데이트가 되지만, 미분을 활용한 업데이트를 하지 않는 텐서들을 등록할 때\n",
    "        # 혹은 차후에 저장해야하는 텐서값 일때 ex) epsilon-탐욕 정책의 epsilon \n",
    "        self.register_buffer('w', torch.zeros(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in mm.children():\n",
    "    print(c)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mm.parameters()\n",
    "\n",
    "mm.parameters() 는 모델 `mm` 안에 있는 파라미터들을 모두 반환해주는 generator 입니다. <br>\n",
    "아마도 `nn.Module`의 기능중 `forward()` 다음으로, 가장 많이 쓰이는 녀석이 될거에요. 이전 실습에서는\n",
    "이 기능을 활용해서, optimizer에 최적화 대상인 파라미터들을 등록해줬었던거 기억하시죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mm.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mm.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.to('device')\n",
    "`pytorch`는 GPU을 활용해 텐서 연산 가속화를 지원합니다. 이때 연산하려는 텐서들은 모두 하나의 장치에 있어야합니다. 예를 들어, 모델의 입력으로 사용 할 텐서 $x$ 는 `cpu` 에 (좀 더 자세히는 `ram`에), 모델의 파라미터들은 `gpu`에 등록되어있다면 $x$와 파라미터를 연산하는 것은 불가능합니다. 이때 간단하게 한 줄로 모델에 등록되어있는 파라미터들을 각각의 장치로 옮기는 것이 가능합니다.\n",
    "\n",
    "> cpu로 모델의 파라미터 옮기기\n",
    "```python\n",
    "model.to('cpu')\n",
    "```\n",
    "\n",
    "> gpu로 모델의 파라미터 옮기기\n",
    "```python\n",
    "model.to('cuda')\n",
    "```\n",
    "\n",
    "주) `cuda`는 그래픽카드 제조사인 Nvidia의 GPGPU 기술의 이름\n",
    "\n",
    "## model.eval(), model.train()\n",
    "모델의 작동하는 방식이, 모델을 평가할 때와 `evaluation (eval)`때와 학습`train`때에 따라 달라지는 경우들이 있습니다. 예를 들어, Q-Learning 에서 train 때는 $\\epsilon$-탐욕적 정책을 활용하지만, 모델의 성능을 평가하고 싶을 때는 탐욕적 정책을 사용하는 경우가 되겠죠? 그런 경우는 어떻게 할 수 있을까요?\n",
    "\n",
    "```python\n",
    "class QLearner(nn.Module):\n",
    "    \n",
    "    def __init__():\n",
    "        ...\n",
    "   \n",
    "    def forward():\n",
    "        if self.train: <- 현재 상태가 'train' 인지 아닌지 알려주는 boolean\n",
    "            # 탐욕적 정책을 구현\n",
    "        else:\n",
    "            # e-탐욕적 정책을 구현\n",
    "```\n",
    "\n",
    "> 모델을 train 모드로 바꾸기\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "\n",
    "> 모델을 eval 모드로 바꾸기\n",
    "```python\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_dim=1, \n",
    "          output_dim=1,\n",
    "          num_neurons=[64],\n",
    "          hidden_act='Identity',\n",
    "          out_act='Identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.state_dict()\n",
    "`model.state_dict()` 는 모델의 파라미터 값 및 buffer의 현재 상태등을 저장해놓은 dictionary 입니다.\n",
    "차후에 <5. 심층강화학습> 자주 사용하게 될건데요. 많은 경우, 훈련중 혹은 훈련이 끝난 모델의 파라미터를 구한 후 디스크에 저장하는 용도로 사용하게 됩니다. 이 경우에는 똑같은 모델 두개를 만들기 위해서 사용해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLP(input_dim=1, \n",
    "           output_dim=1,\n",
    "           num_neurons=[64],\n",
    "           hidden_act='Identity',\n",
    "           out_act='Identity')\n",
    "\n",
    "mlp2.load_state_dict(mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1.0\n",
    "b = 0.5\n",
    "xs, ys = generate_samples(512, w=w, b=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch`의 `Dataset` 및 `Dataloader`로 mini batching 구현하기\n",
    "\n",
    "수업 시간에 배웠던 Mini batching 을 활용하는 Stochastic Gradient Descent (SGD) 기법들을 기억하시나요? 메모리에 Full-batch -우리에게 주어진 데이터 전체- 를 한번에 올리지 못할 때, 전체 데이터를 일부씩 나눠서 경사하강법을 활용해서 학습하는 기법이었죠? 이런 기법을 직접 데이터를 가지고 구현하려면 상당히 번거롭습니다. 물론 강화학습에서는 그 필요성이 다른 경우에 비해 적긴하지만, 그래도 간단하게 알아보고 넘어가도록 하죠.\n",
    "\n",
    "주) SGD가 학습알고리즘이 좀 더 좋은 local 최적점으로 수렴하는데 도움이 된다는 이론적인 연구들이 나오고 있습니다. 이 결과는 실제로 실험을 해봐도 대체로 그런 양상을 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = torch.utils.data.TensorDataset(xs, ys)\n",
    "data_loader = torch.utils.data.DataLoader(ds, batch_size=64)\n",
    "full_loader = torch.utils.data.DataLoader(ds, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 64 # 전체의 데이터셋을 몇번 반복할것인가?\n",
    "opt = torch.optim.Adam(params=mlp.parameters(), lr=1e-3)\n",
    "criteria = torch.nn.MSELoss()\n",
    "\n",
    "sgd_losses = []\n",
    "for e in range(epoch):\n",
    "    for x, y in data_loader:\n",
    "        pred = mlp(x) # forward 를 활용해서 모델의 예측치를 계산\n",
    "        loss = criteria(pred, y) # MSE로 오차계산\n",
    "        \n",
    "        opt.zero_grad() \n",
    "        loss.backward() # loss의 파라미터에 대한 편미분 계산\n",
    "        opt.step() # 경사 하강법을 이용해서 파라미터 개선\n",
    "    \n",
    "    # 전체 데이터셋에 대한 오차 계산\n",
    "    pred = mlp(xs) \n",
    "    loss = criteria(pred, ys)    \n",
    "    # If 'detach()' not applied, then\n",
    "    # it will take so long time until\n",
    "    # Python's garbage collector get rid of redundant information.\n",
    "    loss = loss.detach().numpy() # torch.tensor를 numpy.array로 변환\n",
    "    \n",
    "    # another option (the same effect as applying 'detach()')\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = mlp(xs) \n",
    "        loss = criteria(pred, ys).numpy()\n",
    "    \"\"\"\n",
    "    \n",
    "    sgd_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.grid()\n",
    "ax.plot(sgd_losses)\n",
    "ax.set_xlabel('opt step')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_title('training loss curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과연 SGD가 GD보다 좋은 성능을 보여줄까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, loader, epoch, criteria, xs, ys):\n",
    "    losses = []\n",
    "    for e in range(epoch):\n",
    "        for x, y in loader:\n",
    "            pred = model(x) # forward 를 활용해서 모델의 예측치를 계산\n",
    "            loss = criteria(pred, y) # MSE로 오차계산\n",
    "\n",
    "            opt.zero_grad() \n",
    "            loss.backward() # loss의 파라미터에 대한 편미분 계산\n",
    "            opt.step() # 경사 하강법을 이용해서 파라미터 개선\n",
    "\n",
    "        # 전체 데이터셋에 대한 오차 계산\n",
    "        pred = model(xs)\n",
    "        loss = criteria(pred, ys)\n",
    "        losses.append(loss.detach().numpy())\n",
    "    return losses\n",
    "    \n",
    "def run_minibatch_fullbatch(num_reps : int, \n",
    "                            n_samples : int, \n",
    "                            batch_size : int, \n",
    "                            epoch : int):\n",
    "    criteria = torch.nn.MSELoss()\n",
    "    \n",
    "    sgd_losses = []\n",
    "    gd_losses = []\n",
    "    \n",
    "    for _ in range(num_reps):\n",
    "        mlp = MLP(input_dim=1, \n",
    "                  output_dim=1,\n",
    "                  num_neurons=[64],\n",
    "                  hidden_act='Identity',\n",
    "                  out_act='Identity')\n",
    "        \n",
    "        opt = torch.optim.Adam(params=mlp.parameters(), lr=1e-3)\n",
    "\n",
    "        mlp2 = MLP(input_dim=1, \n",
    "                   output_dim=1,\n",
    "                   num_neurons=[64],\n",
    "                   hidden_act='Identity',\n",
    "                   out_act='Identity')\n",
    "        mlp2.load_state_dict(mlp.state_dict())\n",
    "        opt2 = torch.optim.Adam(params=mlp2.parameters(), lr=1e-3)\n",
    "\n",
    "        xs, ys = generate_samples(n_samples)\n",
    "        ds = torch.utils.data.TensorDataset(xs, ys)\n",
    "        data_loader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "        full_loader = torch.utils.data.DataLoader(ds, batch_size=n_samples)\n",
    "        \n",
    "        # SGD - Mini batch\n",
    "        sgd_loss = train_model(mlp, opt, data_loader, epoch, criteria, xs, ys)        \n",
    "        sgd_losses.append(sgd_loss)\n",
    "        \n",
    "        # GD - Full batch\n",
    "        gd_loss = train_model(mlp2, opt2, full_loader, epoch, criteria, xs, ys)        \n",
    "        gd_losses.append(gd_loss)\n",
    "    \n",
    "    sgd_losses = np.stack(sgd_losses)\n",
    "    gd_losses = np.stack(gd_losses)\n",
    "    return sgd_losses, gd_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_losses, gd_losses = run_minibatch_fullbatch(50, 128, 32, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_loss_mean = np.mean(sgd_losses, axis=0)\n",
    "sgd_loss_std = np.std(sgd_losses, axis=-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_loss_mean = np.mean(gd_losses, axis=0)\n",
    "gd_loss_std = np.std(gd_losses, axis=-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.grid()\n",
    "ax.fill_between(x=range(sgd_loss_mean.shape[0]),\n",
    "                y1=sgd_loss_mean + sgd_loss_std,\n",
    "                y2=sgd_loss_mean - sgd_loss_std,\n",
    "                alpha=0.3)\n",
    "ax.plot(sgd_loss_mean, label='SGD')\n",
    "ax.fill_between(x=range(gd_loss_mean.shape[0]),\n",
    "                y1=gd_loss_mean + gd_loss_std,\n",
    "                y2=gd_loss_mean - gd_loss_std,\n",
    "                alpha=0.3)\n",
    "ax.plot(gd_loss_mean, label='GD')\n",
    "ax.legend()\n",
    "\n",
    "_ = ax.set_xlabel('epoch')\n",
    "_ = ax.set_ylabel('loss')\n",
    "_ = ax.set_title('SGD vs. GD')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
