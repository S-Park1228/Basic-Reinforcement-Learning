{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3216827e",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "---\n",
    "\n",
    "MDP is a mathematical method to define Reinforcement Learning problems. It is generally composed of a tuple, $<\\cal{S}, \\cal{A}, P, R, \\gamma>$. <br>\n",
    "Let us see each elements of MDP. <br>\n",
    "Example: `GridworldEnv` (made by inheriting `discrete.DiscreteEnv` of OpenAI `gym`) <br>\n",
    "OpenAI `gym` is used as the base class for several Reinforcement Learning environments since it provides a standardized interface which is appropriate to be used for Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09dea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0550ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridworld Environment\n",
    "from src.common.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0baafe1",
   "metadata": {},
   "source": [
    "`4 X 4 Gridworld` \n",
    "\n",
    "The visualized MDP environment is as follows.\n",
    "\n",
    "```\n",
    "===========\n",
    "T  x  o  o\n",
    "o  o  o  o\n",
    "o  o  o  o\n",
    "o  o  o  T\n",
    "===========\n",
    "```\n",
    "\n",
    "T: destination (Terminal state) <br>\n",
    "x: current location $s_t$<br>\n",
    "o: points in other environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e907b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_y, num_x = 4, 4\n",
    "# instantiate the environment above\n",
    "env = GridworldEnv(shape =  [num_y, num_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd8830",
   "metadata": {},
   "source": [
    "## State Space $\\cal{S}$ and Action (or State-action) Space $\\cal{A}$\n",
    "\n",
    "The State Space in the above is composed of 16 states divided by a 4 X 4 grid. <br>\n",
    "The Action Space, $\\cal{A}$, consists of the following 4 actions in all states $s$.\n",
    "\n",
    ">`up`, `rihgt`, `down`, `left`  <br>\n",
    "\n",
    "Once an agent moves outside the grid (`action`), its location does not change. <br>\n",
    "For example, if it decides to move `upside` or `to the right` from the highest and the far right state in the grid, then its location does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded340a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: Discrete(16)\n",
      "Number of actions: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(f\"Number of states: {observation_space}\")\n",
    "print(f\"Number of actions: {action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb668763",
   "metadata": {},
   "source": [
    "## State Transition Matrix $P$ and Reward Function $R$\n",
    "\n",
    "A state transition matrix, $P$, in MDP is not a matrix. Rather, it is a `tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3e2d8",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "<img src=\"./images/tensor.png\" width=\"100%\" height=\"50%\" title=\"px(픽셀) 크기 설정\" alt=\"Tensor\"></img>\n",
    "\n",
    "n-dimensional datatype: Rank `n` tensor or `n`-dimensional tensor <br>\n",
    "(`n`: the required number of indices to have access to certain data in a tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11adab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "786b2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a rank-2 tensor\n",
    "num_row = 2\n",
    "num_col = 2\n",
    "rank2_tensor = np.random.random(size = (num_row, num_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd48026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45844718 0.50943114]\n",
      " [0.54694491 0.35885653]]\n"
     ]
    }
   ],
   "source": [
    "# able to infer the number of rank2_tensor's tensor\n",
    "# by counting the number of square brackets\n",
    "print(rank2_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8235d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: (2, 2)\n",
      "Tensor rank: 2\n"
     ]
    }
   ],
   "source": [
    "# numpy_array.shape() returns the number of indices for each dimension.\n",
    "tensor_shape = rank2_tensor.shape\n",
    "tensor_rank = len(tensor_shape)\n",
    "print(f\"Tensor shape: {tensor_shape}\")\n",
    "print(f\"Tensor rank: {tensor_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b175f5",
   "metadata": {},
   "source": [
    "## State Transition Matrix (Tensor) $P$ \n",
    "\n",
    "A state transition matrix, $P$, is a rank-3 tensor. The first axis, the second axis, and the third axis indicate actions ($a$), ($s$), and ($s'$) respectively.\n",
    "\n",
    "Indices 0, 1, 2, 3 are given to action ($a$) and they are movements corresponding to [up, right, down, left] respectively. <br>\n",
    "\n",
    "The number of states ($s$) is 4 x 4 = 16. As such, indices are assigned to have 0, 1, .. , 15. 0 and 1 are the top left and the location to the right by 1 from the location 0. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85fe670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P shape: (4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# loading the state transition tensor P\n",
    "P = env.P_tensor\n",
    "# (the number of actions, the number of states, the number of states)\n",
    "print(f\"P shape: {P.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b030059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the tensor is generated as intended.\n",
    "# Showing an array whose dimension is larger than 2 or 3 is not desirable\n",
    "# so before get started fix an axis.\n",
    "action_up_prob = P[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30823c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(action_up_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13036713",
   "metadata": {},
   "source": [
    "`GridworldEnv` is designed for certain actions to deterministically affect states. This is the special MDP case. . (rather stochastically.. )\n",
    "\n",
    "\n",
    ">That is, all elements (except for a single element) in each row of the state transition matrix, $P$, are all 0.0.\n",
    "\n",
    "\n",
    "In this case, if an action is `up`, then the agent always moves upside. However, in general, actions are not likely to deterministically affect states. Rather, they tend to stochastically have influence on states. For example, the agent might go to other directions even though an actions is `up`.\n",
    "\n",
    "\n",
    "However, for easier understanding of optimal policy and optimal value use deterministic `GridworldEnv` for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fcf94",
   "metadata": {},
   "source": [
    "## Properties of State Transition Matrix\n",
    "\n",
    "Each row of a state transition matrix is the probability that an agent moves from a certain state, $s$, to the next state, $s'$,. Accordingly, every element is not less than 0 and the sum of each row equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff71839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if every element is not less than 0.\n",
    "action_up_prob >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821425ce",
   "metadata": {},
   "source": [
    "Or you can make sure that every element is not less than 0 by using programming (defensive programming). <br>\n",
    ">Defensive Programming: a form of defensive design intended to ensure the continuing function of a piece of software under unforeseen circumstances, often used where high availability, safety, or security is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b046a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_greater_than_0 = action_up_prob >= 0\n",
    "is_all_greater_than_0 = is_greater_than_0.sum() == is_greater_than_0.size\n",
    "is_all_greater_than_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70460a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if the sum of each row equals 1.\n",
    "action_up_prob.sum(axis = 1)\n",
    "# or -> action_up_prob.sum(axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbabf79",
   "metadata": {},
   "source": [
    "## Reward Function $R$ \n",
    "\n",
    "$R$ is a Rank2 tensor (matrix). The vertical and horizontal axes are states and actions respectively.\n",
    "\n",
    "The `GridworldEnv` above receives a reward as much as `-1` no matter where (location) the agent is or no matter which action the agent choose before it arrives at the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e732c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "R = env.R_tensor\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73d084",
   "metadata": {},
   "source": [
    "## Episode of MDP\n",
    "\n",
    "Episodes of the MDP are $<(s_0, a_0, r_0, s_1), (s_1, a_1, r_1, s_2), ..., (s_{t}, a_{t}, r_{t}, s_{t+1}),..., (s_{T-1}, a_{T-1}, r_{T-1}, s_{T})>$. Each tuple indicates the state ($s_t$), the action ($a_t$), the reward ($r_t$), and the next state ($s_{t+1}$) at time $t$. $T$ is the terminal time. \n",
    "\n",
    "However, in general, add the information, `done`, which checks if a certain state is terminal state to each tuple of an episode for RL convenient implementation of Reinforcement Learning algorithms.\n",
    "\n",
    "Let us simulate episodes in the `GridworldEnv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141c5e9",
   "metadata": {},
   "source": [
    "### Policy, $\\pi$, is necessary to simulate MDP.\n",
    "\n",
    "Let us use the policy which selects every action with probability of 0.25 for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a1a5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position index: 13\n"
     ]
    }
   ],
   "source": [
    "# Reset the Gridworld.\n",
    "_ = env.reset()\n",
    "print(f\"Current position index: {env.s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f36fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more intuitive visualization,\n",
    "# replace the indices of actions with directions.\n",
    "action_mapper = {\n",
    "    0: 'UP',\n",
    "    1: 'RIGHT',\n",
    "    2: 'DOWN',\n",
    "    3: 'LEFT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9dbe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At t = 0\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  x  o  T\n",
      "==========\n",
      "state: 13\n",
      "action: UP\n",
      "reward: -1.0\n",
      "the next state: 9\n",
      "\n",
      "At t = 1\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 9\n",
      "action: UP\n",
      "reward: -1.0\n",
      "the next state: 5\n",
      "\n",
      "At t = 2\n",
      "==========\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 5\n",
      "action: RIGHT\n",
      "reward: -1.0\n",
      "the next state: 6\n",
      "\n",
      "At t = 3\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  x  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 6\n",
      "action: DOWN\n",
      "reward: -1.0\n",
      "the next state: 10\n",
      "\n",
      "At t = 4\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  x  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 10\n",
      "action: UP\n",
      "reward: -1.0\n",
      "the next state: 6\n",
      "\n",
      "At t = 5\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  x  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 6\n",
      "action: LEFT\n",
      "reward: -1.0\n",
      "the next state: 5\n",
      "\n",
      "At t = 6\n",
      "==========\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 5\n",
      "action: LEFT\n",
      "reward: -1.0\n",
      "the next state: 4\n",
      "\n",
      "At t = 7\n",
      "==========\n",
      "T  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 4\n",
      "action: RIGHT\n",
      "reward: -1.0\n",
      "the next state: 5\n",
      "\n",
      "At t = 8\n",
      "==========\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 5\n",
      "action: LEFT\n",
      "reward: -1.0\n",
      "the next state: 4\n",
      "\n",
      "At t = 9\n",
      "==========\n",
      "T  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 4\n",
      "action: LEFT\n",
      "reward: -1.0\n",
      "the next state: 4\n",
      "\n",
      "At t = 10\n",
      "==========\n",
      "T  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 4\n",
      "action: DOWN\n",
      "reward: -1.0\n",
      "the next state: 8\n",
      "\n",
      "At t = 11\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 8\n",
      "action: DOWN\n",
      "reward: -1.0\n",
      "the next state: 12\n",
      "\n",
      "At t = 12\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  T\n",
      "==========\n",
      "state: 12\n",
      "action: UP\n",
      "reward: -1.0\n",
      "the next state: 8\n",
      "\n",
      "At t = 13\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 8\n",
      "action: DOWN\n",
      "reward: -1.0\n",
      "the next state: 12\n",
      "\n",
      "At t = 14\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  T\n",
      "==========\n",
      "state: 12\n",
      "action: LEFT\n",
      "reward: -1.0\n",
      "the next state: 12\n",
      "\n",
      "At t = 15\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  T\n",
      "==========\n",
      "state: 12\n",
      "action: UP\n",
      "reward: -1.0\n",
      "the next state: 8\n",
      "\n",
      "At t = 16\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  T\n",
      "==========\n",
      "state: 8\n",
      "action: DOWN\n",
      "reward: -1.0\n",
      "the next state: 12\n",
      "\n",
      "At t = 17\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  T\n",
      "==========\n",
      "state: 12\n",
      "action: RIGHT\n",
      "reward: -1.0\n",
      "the next state: 13\n",
      "\n",
      "At t = 18\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  x  o  T\n",
      "==========\n",
      "state: 13\n",
      "action: RIGHT\n",
      "reward: -1.0\n",
      "the next state: 14\n",
      "\n",
      "At t = 19\n",
      "==========\n",
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  x  T\n",
      "==========\n",
      "state: 14\n",
      "action: RIGHT\n",
      "reward: -1.0\n",
      "the next state: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_counter = 0\n",
    "# Use 'while' since the number of episodes is unknown (random).\n",
    "while True:\n",
    "    print(f\"At t = {step_counter}\")\n",
    "    env._render()\n",
    "    \n",
    "    # current state\n",
    "    cur_state = env.s\n",
    "    # random policy -> an integer in [0, 4)\n",
    "    action = np.random.randint(low = 0, high = 4)\n",
    "    # applying a randomly chosen action by using 'step' method\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    print(f\"state: {cur_state}\")\n",
    "    print(f\"action: {action_mapper[action]}\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    print(f\"the next state: {next_state}\\n\")\n",
    "    step_counter += 1\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7db43e",
   "metadata": {},
   "source": [
    "## Simulating various episodes\n",
    "\n",
    "Even though the state transition matrix ($P$) of the `GridworldEnv` is deterministic, the visited states and the length of each episode can be different (despite of the same starting point) each other due to the stochastic policy function ($\\pi$). Let us simulate these cases too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d08124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, s0):\n",
    "    # Reset the Gridworld.\n",
    "    _ = env.reset()\n",
    "    \n",
    "    step_counter = 0\n",
    "    while True:\n",
    "        action = np.random.randint(low = 0, high = 4)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        step_counter += 1\n",
    "        if done:\n",
    "            break\n",
    "    return step_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05167f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Length of episode: 56\n",
      "Episode 1 | Length of episode: 7\n",
      "Episode 2 | Length of episode: 6\n",
      "Episode 3 | Length of episode: 1\n",
      "Episode 4 | Length of episode: 38\n",
      "Episode 5 | Length of episode: 10\n",
      "Episode 6 | Length of episode: 1\n",
      "Episode 7 | Length of episode: 4\n",
      "Episode 8 | Length of episode: 7\n",
      "Episode 9 | Length of episode: 12\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "s0 = 6\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    len_ep = run_episode(env, s0)\n",
    "    print(f\"Episode {i} | Length of episode: {len_ep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942f90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
